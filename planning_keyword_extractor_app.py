import io
import re
import tempfile
import zipfile
from pathlib import Path
from typing import List

import pandas as pd
import pdfplumber
import streamlit as st

###############################################################################
# Streamlit configuration
###############################################################################

st.set_page_config(
    page_title="Planning PDF Keyword Scanner",
    page_icon="üìÑ",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.title("üìÑ Planning PDF Keyword Scanner ‚Äì Large Batch Edition")

################################################################################
# Sidebar ‚Äì user inputs
################################################################################

st.sidebar.header("üîç Search settings")

# Default keyword set ‚Äì feel free to edit/expand
DEFAULT_KEYWORDS = [
    "construction",
    "height",
    "traffic",
    "parking",
    "heritage",
    "green belt",
    "biodiversity",
    "affordable housing",
]


def _format_default_keywords(kw_list: List[str]) -> str:
    """Return comma-separated keyword string suitable for the text area default."""
    return ", ".join(sorted(set(kw_list)))


keywords_input = st.sidebar.text_area(
    "Keywords to search (comma-separated)",
    value=_format_default_keywords(DEFAULT_KEYWORDS),
    height=120,
    help="Edit the list or paste your own keywords, separated by commas.",
)


def _parse_keywords(s: str) -> List[str]:
    """Convert the textarea value to a clean keyword list.""""
    return [k.strip() for k in s.split(",") if k.strip()]


keywords = _parse_keywords(keywords_input)

st.sidebar.markdown(
    f"**{len(keywords)} keyword(s)** will be searched: {', '.join(keywords)}"
)

# Context window controls
context_window = st.sidebar.slider(
    "Context window (characters before/after match)",
    min_value=20,
    max_value=400,
    value=60,
    step=10,
)

################################################################################
# File uploader ‚Äì now accepts ZIPs containing many PDFs
################################################################################

uploaded_files = st.file_uploader(
    "Upload PDF files **or** ZIP archives (Ctrl/‚åò-click for multi-select)",
    type=["pdf", "zip"],
    accept_multiple_files=True,
)

run_btn = st.button(
    "üöÄ Run scan", disabled=not uploaded_files, type="primary", use_container_width=True
)

################################################################################
# Utility functions
################################################################################


def extract_text_from_pdf(pdf_path: Path) -> str:
    """Return the concatenated text of a PDF file using pdfplumber.""""
    with pdfplumber.open(str(pdf_path)) as pdf:
        return "\n".join(page.extract_text() or "" for page in pdf.pages)


def find_snippets(text: str, keyword: str, window: int) -> List[str]:
    """Return list of context snippets around each keyword match.""""
    pattern = re.compile(re.escape(keyword), re.IGNORECASE)
    snippets: List[str] = []
    for match in pattern.finditer(text):
        start, end = match.span()
        snippet = text[max(0, start - window) : min(len(text), end + window)]
        snippets.append(snippet.replace("\n", " "))
    return snippets


def highlight(snippet: str, keyword: str) -> str:
    """Return snippet as Markdown with **bold** highlights around keyword.""""
    pattern = re.compile(re.escape(keyword), re.IGNORECASE)
    return pattern.sub(lambda m: f"**{m.group(0)}**", snippet)

################################################################################
# Helper ‚Äì gather all PDF paths from uploads (supports ZIP archives)
################################################################################


def gather_pdf_paths(temp_dir: Path) -> List[Path]:
    """Write uploaded files to *temp_dir* and return a list of PDF Paths.""""
    pdf_paths: List[Path] = []

    for uploaded in uploaded_files:
        filename = uploaded.name
        if filename.lower().endswith(".zip"):
            # Treat as archive ‚Äì extract PDFs into temp_dir
            with zipfile.ZipFile(io.BytesIO(uploaded.read())) as zf:
                for member in zf.namelist():
                    if member.lower().endswith(".pdf"):
                        dest = temp_dir / Path(member).name
                        dest.write_bytes(zf.read(member))
                        pdf_paths.append(dest)
        else:
            # Single PDF ‚Äì write into temp_dir
            dest = temp_dir / filename
            dest.write_bytes(uploaded.read())
            pdf_paths.append(dest)

    return pdf_paths

################################################################################
# Main ‚Äì Scan action
################################################################################

if run_btn and uploaded_files:
    with tempfile.TemporaryDirectory() as td:
        temp_dir = Path(td)
        pdf_paths = gather_pdf_paths(temp_dir)

        if not pdf_paths:
            st.warning("No PDFs found inside the uploads.")
            st.stop()

        results = []  # list[dict]
        progress = st.progress(0.0, text="Scanning PDFs‚Ä¶")

        for idx, pdf_path in enumerate(pdf_paths, 1):
            try:
                text = extract_text_from_pdf(pdf_path)
            except Exception as e:
                st.error(f"‚ùå Failed to read {pdf_path.name}: {e}")
                continue

            for kw in keywords:
                snippets = find_snippets(text, kw, context_window)
                for snippet in snippets:
                    results.append(
                        {
                            "File": pdf_path.name,
                            "Keyword": kw,
                            "Snippet": snippet,
                        }
                    )

            progress.progress(idx / len(pdf_paths), text=f"Processed {idx}/{len(pdf_paths)} PDFs")

        progress.empty()

    # Display results after temp_dir is cleaned up (to free space early)
    if not results:
        st.info("No keywords were found in the uploaded documents.")
    else:
        df = pd.DataFrame(results)

        # Highlight snippets once for markdown display
        for kw in keywords:
            mask = df["Keyword"].str.lower() == kw.lower()
            df.loc[mask, "Snippet"] = df.loc[mask, "Snippet"].apply(lambda s, _kw=kw: highlight(s, _kw))

        st.subheader("üóÇÔ∏è Interactive results viewer")

        tabs = st.tabs(["üìë By document", "üîç By keyword", "üìä Raw table"])

        # Tab 1 ‚Äì group by document
        with tabs[0]:
            for file_name, file_df in df.groupby("File"):
                with st.expander(f"{file_name} ‚Äì {len(file_df)} hit(s)"):
                    for keyword, kw_df in file_df.groupby("Keyword"):
                        with st.expander(f"‚ö° {keyword} ‚Äì {len(kw_df)}"):
                            for snippet in kw_df["Snippet"]:
                                st.markdown(f"‚Ä¢ {snippet}", unsafe_allow_html=True)

        # Tab 2 ‚Äì group by keyword
        with tabs[1]:
            for keyword, kw_df in df.groupby("Keyword"):
                with st.expander(f"üîë {keyword} ‚Äì {len(kw_df)} occurrence(s)"):
                    for file_name, file_df in kw_df.groupby("File"):
                        with st.expander(f"üìÑ {file_name} ‚Äì {len(file_df)}"):
                            for snippet in file_df["Snippet"]:
                                st.markdown(f"‚Ä¢ {snippet}", unsafe_allow_html=True)

        # Tab 3 ‚Äì raw table
        with tabs[2]:
            st.dataframe(df, use_container_width=True, height=600)

        # CSV download
        st.download_button(
            label="‚¨áÔ∏è Download results as CSV",
            data=df.to_csv(index=False).encode("utf-8"),
            file_name="keyword_scan_results.csv",
            mime="text/csv",
            use_container_width=True,
        )

################################################################################
# Footer + extra tips for big batches
################################################################################

with st.expander("‚ÑπÔ∏è Tips for processing hundreds of PDFs"):
    st.markdown(
        """
* **Upload ZIP archives** ‚Äì zipping 500 PDFs into a single file avoids browser-side multi-select limits and speeds up the transfer.
* For files >200 MB create a `.streamlit/config.toml` next to this script with:

```toml
[server]
maxUploadSize = 2000  # MB (adjust)
maxMessageSize = 2000
```

* The app streams each PDF to disk before reading it, keeping RAM usage low ‚Äì you can comfortably handle hundreds of typical planning docs on a 1-2 GB instance.
        """
    )

st.markdown(
    """---\n*Built with ‚ù§Ô∏è using Streamlit & pdfplumber. Supports hundreds of PDFs via ZIP upload.*""",
    unsafe_allow_html=True,
)